{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Reddit Data Collection"
      ],
      "metadata": {
        "id": "C5j8YJ4tMWuY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The scripts collects data of reddit posts from various AI-related subreddits using the Reddit API.\n",
        "\n",
        "The data is gathered from multiple subreddits that focus on artificial intelligence, deep learning, machine learning, and related technological advancements.\n",
        "\n",
        "The collection process retrieves posts sorted by 'new' to ensure the dataset contains the latest discussions.\n",
        "\n",
        "**Data Collection Process:**\n",
        "The data is extracted from the following subreddits:\n",
        "\n",
        "* MachineLearning\n",
        "* ArtificialIntelligence\n",
        "* artificial\n",
        "* deeplearning\n",
        "* DeepLearningPapers\n",
        "* datascience\n",
        "* AIethics\n",
        "* AGI\n",
        "* compling\n",
        "* neuralnetworks\n",
        "* learnmachinelearning\n",
        "* CharacterAI\n",
        "* singularity\n",
        "* AI_Agents\n",
        "* technology\n",
        "* ainews\n",
        "* AItoolsCatalog\n",
        "* AI_Tools_Land\n",
        "* tech\n",
        "* Futurology\n",
        "* robotics\n",
        "* computerscience\n",
        "* programming\n",
        "* technews\n",
        "* Automate\n",
        "* Innovation\n",
        "* techsupport\n",
        "* AskTechnology\n",
        "\n",
        "The script fetches posts from these subreddits using the Reddit API.\n",
        "\n",
        "Metadata Collected (CSV Columns & Their Meaning):\n",
        "* title -  The title of the Reddit post.\n",
        "* url – The URL of the post on Reddit.\n",
        "* score – The net score (upvotes minus downvotes) received by the post.\n",
        "* num_awards – The total number of awards given to the post.\n",
        "* created_utc – The timestamp of when the post was created, formatted as YYYY-MM-DD HH:MM:SS.\n",
        "* num_comments – The total number of comments on the post.\n",
        "* subreddit – The subreddit from which the post was collected.\n",
        "* text – The body of the post, if available (otherwise, it remains empty for link-based posts).\n",
        "\n",
        "This dataset is designed to capture trends in AI discussions, monitor engagement, and analyze public perception over time."
      ],
      "metadata": {
        "id": "oT60jyH-Q7Jv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install asyncpraw\n",
        "!pip install nest_asyncio\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmlSrkfIpi2g",
        "outputId": "5df3f32f-09d8-4ab6-8ceb-b3e04a97d416"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting asyncpraw\n",
            "  Downloading asyncpraw-7.8.1-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting aiofiles (from asyncpraw)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: aiohttp<4 in /usr/local/lib/python3.11/dist-packages (from asyncpraw) (3.11.12)\n",
            "Collecting aiosqlite<=0.17.0 (from asyncpraw)\n",
            "  Downloading aiosqlite-0.17.0-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting asyncprawcore<3,>=2.4 (from asyncpraw)\n",
            "  Downloading asyncprawcore-2.4.0-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting update_checker>=0.18 (from asyncpraw)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4->asyncpraw) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4->asyncpraw) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4->asyncpraw) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4->asyncpraw) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4->asyncpraw) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4->asyncpraw) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4->asyncpraw) (1.18.3)\n",
            "Requirement already satisfied: typing_extensions>=3.7.2 in /usr/local/lib/python3.11/dist-packages (from aiosqlite<=0.17.0->asyncpraw) (4.12.2)\n",
            "Requirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from update_checker>=0.18->asyncpraw) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.3.0->update_checker>=0.18->asyncpraw) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.3.0->update_checker>=0.18->asyncpraw) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.3.0->update_checker>=0.18->asyncpraw) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.3.0->update_checker>=0.18->asyncpraw) (2025.1.31)\n",
            "Downloading asyncpraw-7.8.1-py3-none-any.whl (196 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.4/196.4 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiosqlite-0.17.0-py3-none-any.whl (15 kB)\n",
            "Downloading asyncprawcore-2.4.0-py3-none-any.whl (19 kB)\n",
            "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Downloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: aiosqlite, aiofiles, update_checker, asyncprawcore, asyncpraw\n",
            "Successfully installed aiofiles-24.1.0 aiosqlite-0.17.0 asyncpraw-7.8.1 asyncprawcore-2.4.0 update_checker-0.18.0\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LH29NgCaN8n4",
        "outputId": "99c6a8dd-d8c9-4cd0-fae5-315e83f3345c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncpraw\n",
        "import pandas as pd\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Reddit API credentials\n",
        "CLIENT_ID = \"dlAFdiC0gIzdwTHsG4y0AQ\"\n",
        "CLIENT_SECRET = \"97ZIaokro6aH-RIDt7BrNGiSyiL8Tw\"\n",
        "USER_AGENT = 'python:collect-ai-data:v1.0 (by u/MissShik)'\n",
        "\n",
        "reddit = asyncpraw.Reddit(\n",
        "    client_id=CLIENT_ID,\n",
        "    client_secret=CLIENT_SECRET,\n",
        "    user_agent=USER_AGENT\n",
        ")\n",
        "\n",
        "# subreddits list - extract posts from these subreddits only\n",
        "SUBREDDITS = [\n",
        "    \"MachineLearning\", \"ArtificialInteligence\",\n",
        "    \"artificial\", \"deeplearning\",\n",
        "    \"DeepLearningPapers\", \"datascience\",\n",
        "    \"AIethics\", \"AGI\",\n",
        "    \"compling\", \"neuralnetworks\",\n",
        "    \"learnmachinelearning\", \"CharacterAI\",\n",
        "    \"singularity\", \"AI_Agents\",\n",
        "    \"technology\", \"ainews\",\n",
        "    \"AItoolsCatalog\", \"AI_Tools_Land\",\n",
        "    \"tech\",\"Futurology\",\n",
        "    \"robotics\",\"computerscience\",\n",
        "    \"programming\",\"technews\",\n",
        "    \"Automate\",\"Innovation\",\n",
        "    \"techsupport\",\"AskTechnology\"\n",
        "]\n",
        "\n",
        "# collect data retrieval stats\n",
        "class CollectionStats:\n",
        "    def __init__(self):\n",
        "        self.total_queries = 0\n",
        "        self.total_posts = 0\n",
        "        self.start_time = datetime.now()\n",
        "        self.oldest_post_date = None\n",
        "\n",
        "    def update(self, new_posts):\n",
        "        self.total_queries += 1\n",
        "        self.total_posts += len(new_posts)\n",
        "\n",
        "        if new_posts:\n",
        "            oldest_post = min(new_posts, key=lambda x: x['created_utc'])\n",
        "            self.oldest_post_date = oldest_post['created_utc']\n",
        "\n",
        "    def print_stats(self):\n",
        "        duration = (datetime.now() - self.start_time).total_seconds() / 60\n",
        "        print(\"\\n=== Collection Statistics ===\")\n",
        "        print(f\"Duration: {duration:.2f} minutes\")\n",
        "        print(f\"Total queries made: {self.total_queries}\")\n",
        "        print(f\"Total posts collected: {self.total_posts}\")\n",
        "        print(f\"Oldest post date: {self.oldest_post_date}\")\n",
        "        print(f\"Average collection rate: {self.total_queries/duration:.2f} queries per minute\")\n",
        "        print(\"===========================\\n\")\n",
        "\n",
        "unique_post_urls = set()\n",
        "\n",
        "async def fetch_posts(subreddit, stats, days_back=None):\n",
        "    post_data = []\n",
        "    cutoff_date = datetime.now() - timedelta(days=days_back) if days_back else None\n",
        "\n",
        "    try:\n",
        "        post_count = 0\n",
        "        async for post in subreddit.new(limit=None):  # Fetch posts sorted by 'new'\n",
        "            post_date = datetime.fromtimestamp(post.created_utc)\n",
        "\n",
        "            if cutoff_date and post_date < cutoff_date:\n",
        "                print(f\"Reached posts older than {days_back} days\")\n",
        "                break\n",
        "\n",
        "            if post.url in unique_post_urls:\n",
        "                continue\n",
        "\n",
        "            unique_post_urls.add(post.url)\n",
        "\n",
        "            num_awards = post.total_awards_received\n",
        "\n",
        "            post_data.append({\n",
        "                'title': post.title,\n",
        "                'url': post.url,\n",
        "                'score': post.score,  # Net score (upvotes - downvotes) - there is no way to retrieve the actucal upvotes and downvotes\n",
        "                'num_awards': num_awards,\n",
        "                'created_utc': post_date,\n",
        "                'num_comments': post.num_comments,\n",
        "                'subreddit': post.subreddit.display_name,\n",
        "                'text': post.selftext if hasattr(post, 'selftext') else ''\n",
        "            })\n",
        "            post_count += 1\n",
        "\n",
        "        stats.update(post_data[-post_count:] if post_count > 0 else [])\n",
        "        print(f\"Found {post_count} posts in r/{subreddit.display_name}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching posts: {e}\")\n",
        "        await asyncio.sleep(2)\n",
        "\n",
        "    return post_data\n",
        "\n",
        "async def save_to_csv(data,filename):\n",
        "    if data:\n",
        "        df = pd.DataFrame(data)\n",
        "        df = df.sort_values('created_utc', ascending=False)\n",
        "        df['created_utc'] = df['created_utc'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "        save_path = \"/content/drive/My Drive/AITrendAnalysis-project\"\n",
        "        os.makedirs(save_path, exist_ok=True)\n",
        "        full_path = os.path.join(save_path, filename)\n",
        "\n",
        "        df.to_csv(full_path, index=False, mode='w', header=True)\n",
        "        print(f\"Saved {len(df)} rows to {full_path}\")\n",
        "\n",
        "async def collect_data(duration_hours=1, save_threshold=50, days_back=None):\n",
        "    end_time = datetime.now() + timedelta(hours=duration_hours)\n",
        "    all_posts = []\n",
        "    requests_in_window = 0\n",
        "    window_start = datetime.now()\n",
        "    stats = CollectionStats()\n",
        "\n",
        "    print(f\"\\nStarting collection at: {datetime.now()}\")\n",
        "    print(f\"Collection period: {duration_hours} hours\")\n",
        "    print(f\"Collecting posts from the past {days_back if days_back else 'all'} days\")\n",
        "\n",
        "    for subreddit_name in SUBREDDITS:\n",
        "        print(f\"\\nFetching posts from subreddit: r/{subreddit_name}\")\n",
        "        subreddit = await reddit.subreddit(subreddit_name)\n",
        "\n",
        "        while datetime.now() < end_time:\n",
        "          # handle requests limits (due to Reddit API restrictions)\n",
        "            try:\n",
        "                if (datetime.now() - window_start).total_seconds() >= 600:\n",
        "                    requests_in_window = 0\n",
        "                    window_start = datetime.now()\n",
        "\n",
        "                if requests_in_window >= 95:\n",
        "                    wait_time = 600 - (datetime.now() - window_start).total_seconds()\n",
        "                    if wait_time > 0:\n",
        "                        print(f\"Approaching rate limit. Waiting {wait_time:.2f} seconds...\")\n",
        "                        await asyncio.sleep(wait_time)\n",
        "                        requests_in_window = 0\n",
        "                        window_start = datetime.now()\n",
        "\n",
        "                new_posts = await fetch_posts(subreddit, stats, days_back)\n",
        "                if not new_posts:\n",
        "                    print(f\"No new posts found in r/{subreddit_name}. Moving to the next subreddit.\")\n",
        "                    break\n",
        "\n",
        "                all_posts.extend(new_posts)\n",
        "                requests_in_window += 1\n",
        "\n",
        "                if len(all_posts) >= save_threshold:\n",
        "                    await save_to_csv(all_posts, filename=\"reddit-data-02-25_3650_days.csv\")\n",
        "\n",
        "                if (datetime.now() - stats.start_time).total_seconds() % 300 < 1:\n",
        "                    stats.print_stats()\n",
        "\n",
        "                await asyncio.sleep(1)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error during collection: {e}\")\n",
        "                await asyncio.sleep(5)\n",
        "                continue\n",
        "\n",
        "    await save_to_csv(all_posts, filename=\"reddit-data-02-25_3650_days.csv\")\n",
        "    stats.print_stats()\n",
        "    print(f\"\\nCollection completed at: {datetime.now()}\")\n",
        "    print(f\"Total posts collected: {len(all_posts)}\")\n",
        "\n",
        "async def main():\n",
        "    collection_hours = 12\n",
        "    save_threshold = 500\n",
        "    days_back = 3650 # Set to None for all posts - 10 years (past)\n",
        "\n",
        "    try:\n",
        "        await collect_data(\n",
        "            duration_hours=collection_hours,\n",
        "            save_threshold=save_threshold,\n",
        "            days_back=days_back\n",
        "        )\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nCollection interrupted by user. Saving collected data...\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nUnexpected error: {e}\")\n",
        "    finally:\n",
        "        print(\"Script finished.\")\n",
        "\n",
        "loop = asyncio.get_event_loop()\n",
        "loop.run_until_complete(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bfkF05dpVpD",
        "outputId": "8b522302-64ff-4eef-8966-4d5439dcac9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting collection at: 2025-02-22 16:10:21.912900\n",
            "Collection period: 12 hours\n",
            "Collecting posts from the past 3650 days\n",
            "\n",
            "Fetching posts from subreddit: r/MachineLearning\n",
            "Found 881 posts in r/MachineLearning\n",
            "Saved 881 rows to /content/drive/My Drive/AITrendAnalysis-project/reddit-data-02-25_3650_days.csv\n",
            "Found 0 posts in r/MachineLearning\n",
            "No new posts found in r/MachineLearning. Moving to the next subreddit.\n",
            "\n",
            "Fetching posts from subreddit: r/ArtificialInteligence\n",
            "Found 870 posts in r/ArtificialInteligence\n",
            "Saved 1751 rows to /content/drive/My Drive/AITrendAnalysis-project/reddit-data-02-25_3650_days.csv\n",
            "Found 0 posts in r/ArtificialInteligence\n",
            "No new posts found in r/ArtificialInteligence. Moving to the next subreddit.\n",
            "\n",
            "Fetching posts from subreddit: r/artificial\n",
            "Found 922 posts in r/artificial\n",
            "Saved 2673 rows to /content/drive/My Drive/AITrendAnalysis-project/reddit-data-02-25_3650_days.csv\n",
            "Found 0 posts in r/artificial\n",
            "No new posts found in r/artificial. Moving to the next subreddit.\n",
            "\n",
            "Fetching posts from subreddit: r/deeplearning\n",
            "Found 921 posts in r/deeplearning\n",
            "Saved 3594 rows to /content/drive/My Drive/AITrendAnalysis-project/reddit-data-02-25_3650_days.csv\n",
            "Found 0 posts in r/deeplearning\n",
            "No new posts found in r/deeplearning. Moving to the next subreddit.\n",
            "\n",
            "Fetching posts from subreddit: r/DeepLearningPapers\n",
            "Found 865 posts in r/DeepLearningPapers\n",
            "Saved 4459 rows to /content/drive/My Drive/AITrendAnalysis-project/reddit-data-02-25_3650_days.csv\n",
            "Found 0 posts in r/DeepLearningPapers\n",
            "No new posts found in r/DeepLearningPapers. Moving to the next subreddit.\n",
            "\n",
            "Fetching posts from subreddit: r/datascience\n",
            "Found 597 posts in r/datascience\n",
            "Saved 5056 rows to /content/drive/My Drive/AITrendAnalysis-project/reddit-data-02-25_3650_days.csv\n",
            "Found 0 posts in r/datascience\n",
            "No new posts found in r/datascience. Moving to the next subreddit.\n",
            "\n",
            "Fetching posts from subreddit: r/AIethics\n",
            "Found 368 posts in r/AIethics\n",
            "Saved 5424 rows to /content/drive/My Drive/AITrendAnalysis-project/reddit-data-02-25_3650_days.csv\n",
            "Found 0 posts in r/AIethics\n",
            "No new posts found in r/AIethics. Moving to the next subreddit.\n",
            "\n",
            "Fetching posts from subreddit: r/AGI\n",
            "Found 948 posts in r/AGI\n",
            "Saved 6372 rows to /content/drive/My Drive/AITrendAnalysis-project/reddit-data-02-25_3650_days.csv\n",
            "Found 0 posts in r/AGI\n",
            "No new posts found in r/AGI. Moving to the next subreddit.\n",
            "\n",
            "Fetching posts from subreddit: r/compling\n",
            "Reached posts older than 3650 days\n",
            "Found 573 posts in r/compling\n",
            "Saved 6945 rows to /content/drive/My Drive/AITrendAnalysis-project/reddit-data-02-25_3650_days.csv\n",
            "Reached posts older than 3650 days\n",
            "Found 0 posts in r/compling\n",
            "No new posts found in r/compling. Moving to the next subreddit.\n",
            "\n",
            "Fetching posts from subreddit: r/neuralnetworks\n",
            "Found 931 posts in r/neuralnetworks\n",
            "Saved 7876 rows to /content/drive/My Drive/AITrendAnalysis-project/reddit-data-02-25_3650_days.csv\n",
            "Found 0 posts in r/neuralnetworks\n",
            "No new posts found in r/neuralnetworks. Moving to the next subreddit.\n",
            "\n",
            "Fetching posts from subreddit: r/learnmachinelearning\n",
            "Found 931 posts in r/learnmachinelearning\n",
            "Saved 8807 rows to /content/drive/My Drive/AITrendAnalysis-project/reddit-data-02-25_3650_days.csv\n",
            "Found 0 posts in r/learnmachinelearning\n",
            "No new posts found in r/learnmachinelearning. Moving to the next subreddit.\n",
            "\n",
            "Fetching posts from subreddit: r/CharacterAI\n",
            "Found 989 posts in r/CharacterAI\n",
            "Saved 9796 rows to /content/drive/My Drive/AITrendAnalysis-project/reddit-data-02-25_3650_days.csv\n",
            "Found 0 posts in r/CharacterAI\n",
            "No new posts found in r/CharacterAI. Moving to the next subreddit.\n",
            "\n",
            "Fetching posts from subreddit: r/singularity\n",
            "Found 941 posts in r/singularity\n",
            "Saved 10737 rows to /content/drive/My Drive/AITrendAnalysis-project/reddit-data-02-25_3650_days.csv\n",
            "Found 0 posts in r/singularity\n",
            "No new posts found in r/singularity. Moving to the next subreddit.\n",
            "\n",
            "Fetching posts from subreddit: r/AI_Agents\n",
            "Found 923 posts in r/AI_Agents\n",
            "Saved 11660 rows to /content/drive/My Drive/AITrendAnalysis-project/reddit-data-02-25_3650_days.csv\n",
            "Found 0 posts in r/AI_Agents\n",
            "No new posts found in r/AI_Agents. Moving to the next subreddit.\n",
            "\n",
            "Fetching posts from subreddit: r/technology\n",
            "Found 815 posts in r/technology\n",
            "Saved 12475 rows to /content/drive/My Drive/AITrendAnalysis-project/reddit-data-02-25_3650_days.csv\n",
            "Found 0 posts in r/technology\n",
            "No new posts found in r/technology. Moving to the next subreddit.\n",
            "\n",
            "Fetching posts from subreddit: r/ainews\n",
            "Found 30 posts in r/ainews\n",
            "Saved 12505 rows to /content/drive/My Drive/AITrendAnalysis-project/reddit-data-02-25_3650_days.csv\n",
            "Found 0 posts in r/ainews\n",
            "No new posts found in r/ainews. Moving to the next subreddit.\n",
            "\n",
            "Fetching posts from subreddit: r/AItoolsCatalog\n",
            "Found 859 posts in r/AItoolsCatalog\n",
            "Saved 13364 rows to /content/drive/My Drive/AITrendAnalysis-project/reddit-data-02-25_3650_days.csv\n",
            "Found 0 posts in r/AItoolsCatalog\n",
            "No new posts found in r/AItoolsCatalog. Moving to the next subreddit.\n",
            "\n",
            "Fetching posts from subreddit: r/AI_Tools_Land\n",
            "Found 73 posts in r/AI_Tools_Land\n",
            "Saved 13437 rows to /content/drive/My Drive/AITrendAnalysis-project/reddit-data-02-25_3650_days.csv\n",
            "Found 0 posts in r/AI_Tools_Land\n",
            "No new posts found in r/AI_Tools_Land. Moving to the next subreddit.\n",
            "\n",
            "Fetching posts from subreddit: r/tech\n",
            "Found 973 posts in r/tech\n",
            "Saved 14410 rows to /content/drive/My Drive/AITrendAnalysis-project/reddit-data-02-25_3650_days.csv\n",
            "Found 0 posts in r/tech\n",
            "No new posts found in r/tech. Moving to the next subreddit.\n",
            "\n",
            "Fetching posts from subreddit: r/Futurology\n",
            "Found 738 posts in r/Futurology\n",
            "Saved 15148 rows to /content/drive/My Drive/AITrendAnalysis-project/reddit-data-02-25_3650_days.csv\n",
            "Found 0 posts in r/Futurology\n",
            "No new posts found in r/Futurology. Moving to the next subreddit.\n",
            "\n",
            "Fetching posts from subreddit: r/robotics\n",
            "Found 823 posts in r/robotics\n",
            "Saved 15971 rows to /content/drive/My Drive/AITrendAnalysis-project/reddit-data-02-25_3650_days.csv\n",
            "Found 0 posts in r/robotics\n",
            "No new posts found in r/robotics. Moving to the next subreddit.\n",
            "\n",
            "Fetching posts from subreddit: r/computerscience\n",
            "Found 833 posts in r/computerscience\n",
            "Saved 16804 rows to /content/drive/My Drive/AITrendAnalysis-project/reddit-data-02-25_3650_days.csv\n",
            "Found 0 posts in r/computerscience\n",
            "No new posts found in r/computerscience. Moving to the next subreddit.\n",
            "\n",
            "Fetching posts from subreddit: r/programming\n",
            "Found 698 posts in r/programming\n",
            "Saved 17502 rows to /content/drive/My Drive/AITrendAnalysis-project/reddit-data-02-25_3650_days.csv\n",
            "Found 0 posts in r/programming\n",
            "No new posts found in r/programming. Moving to the next subreddit.\n",
            "\n",
            "Fetching posts from subreddit: r/technews\n",
            "Found 809 posts in r/technews\n",
            "Saved 18311 rows to /content/drive/My Drive/AITrendAnalysis-project/reddit-data-02-25_3650_days.csv\n",
            "Found 0 posts in r/technews\n",
            "No new posts found in r/technews. Moving to the next subreddit.\n",
            "\n",
            "Fetching posts from subreddit: r/Automate\n",
            "Found 864 posts in r/Automate\n",
            "Saved 19175 rows to /content/drive/My Drive/AITrendAnalysis-project/reddit-data-02-25_3650_days.csv\n",
            "Found 0 posts in r/Automate\n",
            "No new posts found in r/Automate. Moving to the next subreddit.\n",
            "\n",
            "Fetching posts from subreddit: r/Innovation\n",
            "Found 233 posts in r/Innovation\n",
            "Saved 19408 rows to /content/drive/My Drive/AITrendAnalysis-project/reddit-data-02-25_3650_days.csv\n",
            "Found 0 posts in r/Innovation\n",
            "No new posts found in r/Innovation. Moving to the next subreddit.\n",
            "\n",
            "Fetching posts from subreddit: r/techsupport\n",
            "Found 986 posts in r/techsupport\n",
            "Saved 20394 rows to /content/drive/My Drive/AITrendAnalysis-project/reddit-data-02-25_3650_days.csv\n",
            "Found 1 posts in r/techsupport\n",
            "Saved 20395 rows to /content/drive/My Drive/AITrendAnalysis-project/reddit-data-02-25_3650_days.csv\n",
            "Found 0 posts in r/techsupport\n",
            "No new posts found in r/techsupport. Moving to the next subreddit.\n",
            "\n",
            "Fetching posts from subreddit: r/AskTechnology\n",
            "Found 977 posts in r/AskTechnology\n",
            "Saved 21372 rows to /content/drive/My Drive/AITrendAnalysis-project/reddit-data-02-25_3650_days.csv\n",
            "Found 0 posts in r/AskTechnology\n",
            "No new posts found in r/AskTechnology. Moving to the next subreddit.\n",
            "Saved 21372 rows to /content/drive/My Drive/AITrendAnalysis-project/reddit-data-02-25_3650_days.csv\n",
            "\n",
            "=== Collection Statistics ===\n",
            "Duration: 10.70 minutes\n",
            "Total queries made: 57\n",
            "Total posts collected: 21372\n",
            "Oldest post date: 2024-11-09 17:55:17\n",
            "Average collection rate: 5.33 queries per minute\n",
            "===========================\n",
            "\n",
            "\n",
            "Collection completed at: 2025-02-22 16:21:03.962535\n",
            "Total posts collected: 21372\n",
            "Script finished.\n"
          ]
        }
      ]
    }
  ]
}